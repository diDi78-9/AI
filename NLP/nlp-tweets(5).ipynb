{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":2468672,"sourceType":"datasetVersion","datasetId":1455358}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Import Data","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n\nfrom xgboost import XGBClassifier\n\nfrom transformers import AutoTokenizer,AutoModel\nimport torch\n\nfrom sklearn.svm import SVC\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom collections import Counter\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntrain_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"EDA Analysis","metadata":{}},{"cell_type":"code","source":"# Plot target distribution\nplt.figure(figsize=(6, 4))\nsns.countplot(data=train_data, x='target', palette='viridis')\nplt.title('Target Distribution')\nplt.xlabel('Target')\nplt.ylabel('Count')\nplt.show()\n\n#Data\n#0 - not related to disaster\n#1 - related to disaster","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to generate a word cloud\ndef generate_wordcloud(text, title):\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n    plt.figure(figsize=(10, 6))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title, fontsize=16)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate word cloud for all text data\nall_text = ' '.join(train_data['text'].astype(str))\ngenerate_wordcloud(all_text, 'Word Cloud for All Tweets')\n\n# Generate word cloud for each target class\nfor target in train_data['target'].unique():\n    target_text = ' '.join(train_data[train_data['target'] == target]['text'].astype(str))\n    generate_wordcloud(target_text, f'Word Cloud for Target: {target}')\n\n# Analyze frequent words\ndef most_common_words(text, num_words=10):\n    words = text.split()\n    word_counts = Counter(words)\n    return word_counts.most_common(num_words)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clean text\ntrain_data[\"clean_text\"] = train_data[\"text\"].str.replace(r\"[^a-zA-Z0-9\\s]\", \"\", regex=True).str.lower().str.strip()\ntest_data[\"clean_text\"] = test_data[\"text\"].str.replace(r\"[^a-zA-Z0-9\\s]\", \"\", regex=True).str.lower().str.strip()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# \n\nBERTs Transformer Model","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nMAX_VOCAB_SIZE = 20000\nMAX_SEQUENCE_LENGTH = 100\nEMBEDDING_DIM = 100\n\n# Load BERT Tokenizer & Model\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/transformers/bert-base-uncased\")\nbert_model = AutoModel.from_pretrained(\"/kaggle/input/transformers/bert-base-uncased\")\n\n# Extract BERT Embeddings\ndef get_bert_embeddings(texts):\n    inputs = tokenizer(texts, padding=True, truncation=True, max_length=MAX_SEQUENCE_LENGTH, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    return outputs.last_hidden_state[:, 0, :].numpy()\n\n# Apply BERT Embeddings\nX_bert_train = get_bert_embeddings(train_data[\"clean_text\"].tolist())\nX_bert_test = get_bert_embeddings(test_data[\"clean_text\"].tolist())\ny = np.array(train_data[\"target\"])\n\n# Class Weight Handling\nclasses = np.unique(y)\nclass_weights = compute_class_weight('balanced', classes=classes, y=y)\nclass_weight_dict = dict(zip(classes, class_weights))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(X_bert_train, y, test_size=0.2, random_state=42)\n\n# Reshape for LSTM\nX_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\nX_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build LSTM Model\nlstm_model = Sequential([\n    Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n    Bidirectional(LSTM(256, return_sequences=True)),\n    Dropout(0.4),\n    LSTM(256),\n    Dropout(0.4),\n    Dense(128, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nlstm_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nlstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), class_weight=class_weight_dict)\n\n# Extract LSTM Features\nX_train_lstm_features = lstm_model.predict(X_train).reshape(X_train.shape[0], -1)\nX_test_lstm_features = lstm_model.predict(X_test).reshape(X_test.shape[0], -1)\n\n# Train XGBoost Model\nxgb_model = XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=10, random_state=42)\nxgb_model.fit(X_train_lstm_features, y_train)\ny_pred_xgb = xgb_model.predict(X_test_lstm_features)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate XGBoost\nprint(f\"Hybrid BERT + LSTM + XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\nprint(\"\\nClassification Report - XGBoost (BERT + LSTM Features)\")\nprint(classification_report(y_test, y_pred_xgb))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Additional Evaluation - F1 Score\nprint(f\"F1 Score (Macro): {f1_score(y_test, y_pred_xgb, average='macro'):.4f}\")\n\n# Confusion Matrix Visualization\ncm = confusion_matrix(y_test, y_pred_xgb)\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix - XGBoost (BERT + LSTM Features)\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}